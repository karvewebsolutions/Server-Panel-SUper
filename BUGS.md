# Known Bugs in Restore Workflow

- **Data loss risk when restoration input is invalid:** `restart_app_instance` removes the existing runtime data directory before verifying that the supplied `restore_dir` exists and can be copied. If `shutil.copytree` fails (for example, because the restore directory is missing or unreadable), the original data is already deleted and the app container has been removed, leaving the instance down with its data wiped. The code removes `data_dir` and then immediately copies from `restore_dir` without validation or a fallback. (See `backend/app/services/deployment_engine.py`, lines 108-116.)
- **Restore fails when container is already absent:** `restart_app_instance` unconditionally stops and removes the container. When a restore is triggered for an instance whose container is already removed or missing (e.g., after a crash or manual cleanup), `stop_container` raises and the restore aborts before data is applied. There is no guard to skip stop/remove when the container does not exist. (See `backend/app/services/deployment_engine.py`, lines 102-107.)
- **No error status on failed restart:** Unlike `deploy_app_instance`, `restart_app_instance` does not catch exceptions to record an "error" status on the `AppInstance`. If container startup fails after deleting the previous container or data directory, the database state remains "stopped" or stale, obscuring that the restore/restart failed. (Compare `backend/app/services/deployment_engine.py`, lines 65-71 vs. 90-143.)
